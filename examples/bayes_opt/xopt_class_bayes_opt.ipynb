{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the class\n",
    "from xopt import Xopt\n",
    "from botorch.test_functions.multi_fidelity import AugmentedHartmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see log messages\n",
    "from xopt import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Xopt` object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.\n",
    "\n",
    "Here we will make one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a proper input file. \n",
    "YAML = \"\"\"\n",
    "xopt: {output_path: null}\n",
    "\n",
    "algorithm:\n",
    "  name: bayesian_optimization\n",
    "  options:  \n",
    "      n_initial_samples: 16\n",
    "      n_steps: 6\n",
    "      generator_options:\n",
    "          acquisition_function: custom_acq.acq\n",
    "          use_gpu: False\n",
    "\n",
    "simulation: \n",
    "  name: test_multi_fidelity\n",
    "  evaluate: xopt.tests.evaluators.multi_fidelity.evaluate\n",
    "\n",
    "vocs:\n",
    "  name: test_multi_fidelity\n",
    "  description: null\n",
    "  simulation: test_multi_fidelity\n",
    "  templates: null\n",
    "  variables:\n",
    "    x1: [0, 1.0]\n",
    "    x2: [0, 1.0]\n",
    "    x3: [0, 1.0]\n",
    "    x4: [0, 1.0]\n",
    "    x5: [0, 1.0]\n",
    "    x6: [0, 1.0]\n",
    "    cost: [0, 1.0]                          ## NOTE: THIS IS REQUIRED FOR MULTI-FIDELITY OPTIMIZATION\n",
    "  objectives:\n",
    "    y1: 'MINIMIZE'\n",
    "  linked_variables: {}\n",
    "  constants: {a: dummy_constant}\n",
    "\n",
    "\"\"\"\n",
    "config = YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from text\n",
      "Loading config from dict.\n",
      "Loading config from dict.\n",
      "Loading config from dict.\n",
      "Loading config from dict.\n",
      "`name` keyword no longer allowed in vocs config, removing\n",
      "`description` keyword no longer allowed in vocs config, removing\n",
      "`simulation` keyword no longer allowed in vocs config, removing\n",
      "`templates` keyword no longer allowed in vocs config, moving to simulation `options`\n",
      "Warning: No path set for key xopt : output_path\n",
      "Warning: No path set for key algorithm : options : restart_file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "            Xopt \n",
       "________________________________           \n",
       "Version: 0.4.3+183.g2fe3819.dirty\n",
       "Configured: True\n",
       "Config as YAML:\n",
       "xopt: {output_path: null}\n",
       "algorithm:\n",
       "  name: bayesian_optimization\n",
       "  options:\n",
       "    n_initial_samples: 16\n",
       "    n_steps: 6\n",
       "    verbose: true\n",
       "    generator_options: {acquisition_function: !!python/name:custom_acq.acq '', use_gpu: false}\n",
       "    custom_model: null\n",
       "    restart_file: null\n",
       "    initial_x: null\n",
       "  function: xopt.bayesian.algorithms.bayesian_optimize\n",
       "simulation:\n",
       "  name: test_multi_fidelity\n",
       "  evaluate: xopt.tests.evaluators.multi_fidelity.evaluate\n",
       "  options: {templates: null, extra_option: abc}\n",
       "vocs:\n",
       "  variables:\n",
       "    x1: [0, 1.0]\n",
       "    x2: [0, 1.0]\n",
       "    x3: [0, 1.0]\n",
       "    x4: [0, 1.0]\n",
       "    x5: [0, 1.0]\n",
       "    x6: [0, 1.0]\n",
       "    cost: [0, 1.0]\n",
       "  objectives: {y1: MINIMIZE}\n",
       "  linked_variables: null\n",
       "  constants: {a: dummy_constant}\n",
       "  constraints: null"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Xopt(config)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of these\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "#from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "executor = PoolExecutor()\n",
    "# This will also work. \n",
    "#executor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at time 2021-09-28T15:20:23-07:00\n",
      "started running optimization with generator: <xopt.bayesian.generators.generator.BayesianGenerator object at 0x7fa9158ddb20>\n",
      "submitting initial candidates at time 2021-09-28T15:20:23-07:00\n",
      "starting optimization loop\n",
      "submitting candidates at time 2021-09-28T15:20:24-07:00\n",
      "submitting candidates at time 2021-09-28T15:20:25-07:00\n",
      "submitting candidates at time 2021-09-28T15:20:26-07:00\n",
      "submitting candidates at time 2021-09-28T15:20:27-07:00\n",
      "submitting candidates at time 2021-09-28T15:20:28-07:00\n",
      "submitting candidates at time 2021-09-28T15:20:28-07:00\n"
     ]
    }
   ],
   "source": [
    "# Change max generations\n",
    "X.run(executor=executor)\n",
    "results = X.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get highest fidelity global optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator object\n",
    "from xopt.bayesian.generators.multi_fidelity import MultiFidelityGenerator\n",
    "from xopt.bayesian.models.models import create_multi_fidelity_model\n",
    "\n",
    "gen = MultiFidelityGenerator(X.vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'corrected_constraints'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/wj/lfgr01993dx79p9cm_skykbw0000gn/T/ipykernel_62964/2151074823.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# create model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_multi_fidelity_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'variables'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'corrected_objectives'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'corrected_constraints'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m: 'corrected_constraints'"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = create_multi_fidelity_model(results['variables'], results['corrected_objectives'], results['corrected_constraints'], X.vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: we want to get the minimum evaluated at the highest fidelity -> make sure to use get_recommendation\n",
    "rec = gen.get_recommendation(model)\n",
    "problem = AugmentedHartmann(negate=False)\n",
    "problem(rec) ## NOTE: the correct global minimum is -3.32237"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}