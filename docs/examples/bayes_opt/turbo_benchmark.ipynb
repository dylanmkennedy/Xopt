{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking TuRBO Bayesian Optimization\n",
    "In this tutorial we demonstrate the use of Xopt to preform Bayesian Optimization on\n",
    "the 20D Ackley test function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the test problem\n",
    "Here we define a simple optimization problem, where we attempt to minimize the sin\n",
    "function in the domian [0,2*pi]. Note that the function used to evaluate the\n",
    "objective function takes a dictionary as input and returns a dictionary as the output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from xopt.resources.test_functions.sphere_20 import vocs, evaluate_sphere\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Xopt objects\n",
    "Create the evaluator to evaluate our test function and create a generator that uses\n",
    "the Upper Confidence Bound acqusition function to perform Bayesian Optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from xopt.evaluator import Evaluator\n",
    "from xopt.generators.bayesian import TuRBOUpperConfidenceBoundGenerator\n",
    "from xopt import Xopt\n",
    "\n",
    "evaluator = Evaluator(function=evaluate_sphere)\n",
    "generator = TuRBOUpperConfidenceBoundGenerator(vocs)\n",
    "generator.options.n_initial = 5\n",
    "generator.options.optim.num_restarts=20\n",
    "X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate and evaluate initial points\n",
    "To begin optimization, we must generate some random initial data points. The first call\n",
    "to `X.step()` will generate and evaluate a number of randomly points specified by the\n",
    " generator. Note that if we add data to xopt before calling `X.step()` by assigning\n",
    " the data to `X.data`, calls to `X.step()` will ignore the random generation and\n",
    " proceed to generating points via Bayesian optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "         x0        x1        x2        x3        x4        x5        x6  \\\n1  0.062995 -0.062279  0.544839 -0.503431  0.821776  0.629965 -0.474384   \n2 -0.561243  0.512332 -0.632823  0.350470 -0.523234  0.384745 -0.785675   \n3 -0.901420 -0.238834  0.060313  0.320931  0.515013 -0.946428  0.467778   \n4 -0.368522 -0.159474  0.759957  0.700507 -0.905647  0.287221 -0.391568   \n5  0.551577 -0.984087 -0.566301  0.200010  0.372020 -0.720478 -0.538366   \n\n         x7        x8        x9  ...       x13       x14       x15       x16  \\\n1 -0.033765 -0.387065 -0.940502  ... -0.615613 -0.160023 -0.488284 -0.947566   \n2 -0.149247 -0.831706 -0.579351  ... -0.314386 -0.732740  0.657981 -0.765338   \n3  0.294698 -0.681843  0.671658  ...  0.524845  0.225160  0.236900 -0.977615   \n4 -0.171503 -0.177241  0.405613  ... -0.494040  0.322425 -0.006682  0.390137   \n5 -0.699903 -0.799647 -0.825138  ... -0.529741 -0.016578  0.406307  0.833492   \n\n        x17       x18       x19          f  xopt_runtime  xopt_error  \n1  0.328403 -0.366973  0.738352  6.1677294      0.000156       False  \n2  0.152458  0.349693  0.601206  6.9899263      0.000030       False  \n3  0.674749 -0.572095 -0.380031  6.0905423      0.000025       False  \n4 -0.551452 -0.388566  0.240009  5.3742104      0.000034       False  \n5  0.959548 -0.175618 -0.134072  7.0636697      0.000024       False  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0</th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>x4</th>\n      <th>x5</th>\n      <th>x6</th>\n      <th>x7</th>\n      <th>x8</th>\n      <th>x9</th>\n      <th>...</th>\n      <th>x13</th>\n      <th>x14</th>\n      <th>x15</th>\n      <th>x16</th>\n      <th>x17</th>\n      <th>x18</th>\n      <th>x19</th>\n      <th>f</th>\n      <th>xopt_runtime</th>\n      <th>xopt_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.062995</td>\n      <td>-0.062279</td>\n      <td>0.544839</td>\n      <td>-0.503431</td>\n      <td>0.821776</td>\n      <td>0.629965</td>\n      <td>-0.474384</td>\n      <td>-0.033765</td>\n      <td>-0.387065</td>\n      <td>-0.940502</td>\n      <td>...</td>\n      <td>-0.615613</td>\n      <td>-0.160023</td>\n      <td>-0.488284</td>\n      <td>-0.947566</td>\n      <td>0.328403</td>\n      <td>-0.366973</td>\n      <td>0.738352</td>\n      <td>6.1677294</td>\n      <td>0.000156</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.561243</td>\n      <td>0.512332</td>\n      <td>-0.632823</td>\n      <td>0.350470</td>\n      <td>-0.523234</td>\n      <td>0.384745</td>\n      <td>-0.785675</td>\n      <td>-0.149247</td>\n      <td>-0.831706</td>\n      <td>-0.579351</td>\n      <td>...</td>\n      <td>-0.314386</td>\n      <td>-0.732740</td>\n      <td>0.657981</td>\n      <td>-0.765338</td>\n      <td>0.152458</td>\n      <td>0.349693</td>\n      <td>0.601206</td>\n      <td>6.9899263</td>\n      <td>0.000030</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.901420</td>\n      <td>-0.238834</td>\n      <td>0.060313</td>\n      <td>0.320931</td>\n      <td>0.515013</td>\n      <td>-0.946428</td>\n      <td>0.467778</td>\n      <td>0.294698</td>\n      <td>-0.681843</td>\n      <td>0.671658</td>\n      <td>...</td>\n      <td>0.524845</td>\n      <td>0.225160</td>\n      <td>0.236900</td>\n      <td>-0.977615</td>\n      <td>0.674749</td>\n      <td>-0.572095</td>\n      <td>-0.380031</td>\n      <td>6.0905423</td>\n      <td>0.000025</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.368522</td>\n      <td>-0.159474</td>\n      <td>0.759957</td>\n      <td>0.700507</td>\n      <td>-0.905647</td>\n      <td>0.287221</td>\n      <td>-0.391568</td>\n      <td>-0.171503</td>\n      <td>-0.177241</td>\n      <td>0.405613</td>\n      <td>...</td>\n      <td>-0.494040</td>\n      <td>0.322425</td>\n      <td>-0.006682</td>\n      <td>0.390137</td>\n      <td>-0.551452</td>\n      <td>-0.388566</td>\n      <td>0.240009</td>\n      <td>5.3742104</td>\n      <td>0.000034</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.551577</td>\n      <td>-0.984087</td>\n      <td>-0.566301</td>\n      <td>0.200010</td>\n      <td>0.372020</td>\n      <td>-0.720478</td>\n      <td>-0.538366</td>\n      <td>-0.699903</td>\n      <td>-0.799647</td>\n      <td>-0.825138</td>\n      <td>...</td>\n      <td>-0.529741</td>\n      <td>-0.016578</td>\n      <td>0.406307</td>\n      <td>0.833492</td>\n      <td>0.959548</td>\n      <td>-0.175618</td>\n      <td>-0.134072</td>\n      <td>7.0636697</td>\n      <td>0.000024</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 23 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate random initial points\n",
    "X.step()\n",
    "\n",
    "# inspect the gathered data\n",
    "X.data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n         -1., -1., -1., -1., -1., -1.],\n        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n          1.,  1.,  1.,  1.,  1.,  1.]], dtype=torch.float64)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine trust region from gathered data\n",
    "generator.train_model()\n",
    "generator.get_trust_region()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Do bayesian optimization steps\n",
    "To perform optimization we simply call `X.step()` in a loop. This allows us to do\n",
    "intermediate tasks in between optimization steps, such as examining the model and\n",
    "acquisition function at each step (as we demonstrate here)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0.5, sc:0, fc:0,best_val: inf\n",
      "0.5, sc:1, fc:0,best_val: 7.063669681549072\n",
      "0.5, sc:2, fc:0,best_val: 5.595542907714844\n",
      "0.5, sc:3, fc:0,best_val: 5.253245830535889\n",
      "0.5, sc:4, fc:0,best_val: 3.20282244682312\n",
      "0.5, sc:0, fc:1,best_val: 3.20282244682312\n",
      "0.5, sc:1, fc:0,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:1,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:2,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:3,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:4,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:5,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:6,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:7,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:8,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:9,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:10,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:11,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:12,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:13,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:14,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:15,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:16,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:17,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:18,best_val: 2.684757947921753\n",
      "0.5, sc:0, fc:19,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:0,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:1,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:2,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:3,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:4,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:5,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:6,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:7,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:8,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:9,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:10,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:11,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:12,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:13,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:14,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:15,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:16,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:17,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:18,best_val: 2.684757947921753\n",
      "0.25, sc:0, fc:19,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:0,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:1,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:2,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:3,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:4,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:5,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:6,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:7,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:8,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:9,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:10,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:11,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:12,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:13,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:14,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:15,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:16,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:17,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:18,best_val: 2.684757947921753\n",
      "0.125, sc:0, fc:19,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:0,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:1,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:2,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:3,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:4,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:5,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:6,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:7,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:8,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:9,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:10,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:11,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:12,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:13,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:14,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:15,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:16,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:17,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:18,best_val: 2.684757947921753\n",
      "0.0625, sc:0, fc:19,best_val: 2.684757947921753\n",
      "0.03125, sc:0, fc:0,best_val: 2.684757947921753\n",
      "0.03125, sc:0, fc:1,best_val: 2.684757947921753\n",
      "0.03125, sc:1, fc:0,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:1,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:2,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:3,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:4,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:5,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:6,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:7,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:8,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:9,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:10,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:11,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:12,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:13,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:14,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:15,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:16,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:17,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:18,best_val: 2.633711814880371\n",
      "0.03125, sc:0, fc:19,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:0,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:1,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:2,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:3,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:4,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:5,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:6,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:7,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:8,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:9,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:10,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:11,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:12,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:13,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:14,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:15,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:16,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:17,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:18,best_val: 2.633711814880371\n",
      "0.015625, sc:0, fc:19,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:0,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:1,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:2,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:3,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:4,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:5,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:6,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:7,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:8,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:9,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:10,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:11,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:12,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:13,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:14,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:15,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:16,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:17,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:18,best_val: 2.633711814880371\n",
      "0.0078125, sc:0, fc:19,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:0,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:1,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:2,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:3,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:4,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:5,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:6,best_val: 2.633711814880371\n",
      "0.00390625, sc:0, fc:7,best_val: 2.633711814880371\n",
      "0.00390625, sc:1, fc:0,best_val: 2.5373172760009766\n",
      "0.00390625, sc:0, fc:1,best_val: 2.5373172760009766\n",
      "0.00390625, sc:0, fc:2,best_val: 2.5373172760009766\n",
      "0.00390625, sc:0, fc:3,best_val: 2.5373172760009766\n",
      "0.00390625, sc:0, fc:4,best_val: 2.5373172760009766\n",
      "0.00390625, sc:0, fc:5,best_val: 2.5373172760009766\n",
      "0.00390625, sc:0, fc:6,best_val: 2.5373172760009766\n",
      "0.00390625, sc:0, fc:7,best_val: 2.5373172760009766\n",
      "0.00390625, sc:1, fc:0,best_val: 2.204240322113037\n",
      "0.00390625, sc:0, fc:1,best_val: 2.204240322113037\n",
      "0.00390625, sc:1, fc:0,best_val: 2.052473545074463\n",
      "0.00390625, sc:0, fc:1,best_val: 2.052473545074463\n",
      "0.00390625, sc:1, fc:0,best_val: 1.789936900138855\n",
      "0.00390625, sc:2, fc:0,best_val: 0.17120537161827087\n",
      "0.00390625, sc:3, fc:0,best_val: 0.08065520972013474\n",
      "0.00390625, sc:0, fc:1,best_val: 0.08065520972013474\n",
      "0.00390625, sc:0, fc:2,best_val: 0.08065520972013474\n",
      "0.00390625, sc:0, fc:3,best_val: 0.08065520972013474\n",
      "0.00390625, sc:1, fc:0,best_val: 0.05768460035324097\n",
      "0.00390625, sc:0, fc:1,best_val: 0.05768460035324097\n",
      "0.00390625, sc:0, fc:2,best_val: 0.05768460035324097\n",
      "0.00390625, sc:0, fc:3,best_val: 0.05768460035324097\n",
      "0.00390625, sc:0, fc:4,best_val: 0.05768460035324097\n",
      "0.00390625, sc:0, fc:5,best_val: 0.05768460035324097\n",
      "0.00390625, sc:1, fc:0,best_val: 0.044483330100774765\n",
      "0.00390625, sc:0, fc:1,best_val: 0.044483330100774765\n",
      "0.00390625, sc:0, fc:2,best_val: 0.044483330100774765\n",
      "0.00390625, sc:1, fc:0,best_val: 0.02802077867090702\n",
      "0.00390625, sc:0, fc:1,best_val: 0.02802077867090702\n",
      "0.00390625, sc:0, fc:2,best_val: 0.02802077867090702\n",
      "0.00390625, sc:0, fc:3,best_val: 0.02802077867090702\n",
      "0.00390625, sc:1, fc:0,best_val: 0.022774383425712585\n",
      "0.00390625, sc:0, fc:1,best_val: 0.022774383425712585\n",
      "0.00390625, sc:1, fc:0,best_val: 0.016229726374149323\n",
      "0.00390625, sc:0, fc:1,best_val: 0.016229726374149323\n",
      "0.00390625, sc:1, fc:0,best_val: 0.010348857380449772\n",
      "0.00390625, sc:2, fc:0,best_val: 0.009491454809904099\n",
      "0.00390625, sc:0, fc:1,best_val: 0.009491454809904099\n",
      "0.00390625, sc:1, fc:0,best_val: 0.007626265753060579\n",
      "0.00390625, sc:0, fc:1,best_val: 0.007626265753060579\n",
      "0.00390625, sc:0, fc:2,best_val: 0.007626265753060579\n",
      "0.00390625, sc:0, fc:3,best_val: 0.007626265753060579\n",
      "0.00390625, sc:0, fc:4,best_val: 0.007626265753060579\n",
      "0.00390625, sc:0, fc:5,best_val: 0.007626265753060579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(X.generator.turbo_state.failure_tolerance)\n",
    "for i in range(200):\n",
    "    print(f\"{X.generator.turbo_state.length}, \"\n",
    "          f\"sc:{X.generator.turbo_state.success_counter}, \"\n",
    "          f\"fc:{X.generator.turbo_state.failure_counter},\"\n",
    "          f\"best_val: {X.generator.turbo_state.best_value}\"\n",
    "          )\n",
    "    # do the optimization step\n",
    "    X.step()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "           x0        x1        x2        x3        x4        x5        x6  \\\n1    0.062995 -0.062279  0.544839 -0.503431  0.821776  0.629965 -0.474384   \n2   -0.561243  0.512332 -0.632823  0.350470 -0.523234  0.384745 -0.785675   \n3   -0.901420 -0.238834  0.060313  0.320931  0.515013 -0.946428  0.467778   \n4   -0.368522 -0.159474  0.759957  0.700507 -0.905647  0.287221 -0.391568   \n5    0.551577 -0.984087 -0.566301  0.200010  0.372020 -0.720478 -0.538366   \n..        ...       ...       ...       ...       ...       ...       ...   \n201 -0.027026  0.035315 -0.030245  0.030285  0.020834 -0.016995  0.009642   \n202  0.006212  0.035013  0.012527 -0.003776  0.022976  0.016143 -0.014786   \n203 -0.026574  0.034857  0.012320  0.003896  0.022817  0.015985 -0.023450   \n204 -0.017086  0.034640 -0.022766  0.029541 -0.007904  0.015767 -0.017971   \n205 -0.017817  0.034497  0.011844  0.016088  0.022448  0.015622  0.011351   \n\n           x7        x8        x9  ...       x13       x14       x15  \\\n1   -0.033765 -0.387065 -0.940502  ... -0.615613 -0.160023 -0.488284   \n2   -0.149247 -0.831706 -0.579351  ... -0.314386 -0.732740  0.657981   \n3    0.294698 -0.681843  0.671658  ...  0.524845  0.225160  0.236900   \n4   -0.171503 -0.177241  0.405613  ... -0.494040  0.322425 -0.006682   \n5   -0.699903 -0.799647 -0.825138  ... -0.529741 -0.016578  0.406307   \n..        ...       ...       ...  ...       ...       ...       ...   \n201  0.018554  0.019674 -0.018592  ... -0.021082  0.028225 -0.024120   \n202  0.018250 -0.009620  0.010238  ...  0.002984 -0.004988 -0.037240   \n203 -0.013961  0.019353  0.010468  ... -0.029400  0.027764 -0.024597   \n204  0.017889  0.019539  0.012732  ...  0.002497 -0.004412 -0.043894   \n205  0.017745 -0.003923  0.001219  ...  0.002310  0.027400 -0.027870   \n\n          x16       x17       x18       x19            f  xopt_runtime  \\\n1   -0.947566  0.328403 -0.366973  0.738352    6.1677294      0.000156   \n2   -0.765338  0.152458  0.349693  0.601206    6.9899263      0.000030   \n3   -0.977615  0.674749 -0.572095 -0.380031    6.0905423      0.000025   \n4    0.390137 -0.551452 -0.388566  0.240009    5.3742104      0.000034   \n5    0.833492  0.959548 -0.175618 -0.134072    7.0636697      0.000024   \n..        ...       ...       ...       ...          ...           ...   \n201 -0.003468 -0.010130  0.020217 -0.041383  0.012211022      0.000061   \n202 -0.003862 -0.016839 -0.001926 -0.041088  0.008816749      0.000055   \n203 -0.004058 -0.011925  0.025997 -0.009412    0.0097027      0.000047   \n204 -0.004313 -0.026123 -0.003168 -0.040722  0.010275573      0.000067   \n205 -0.004496 -0.031345 -0.003008 -0.009764  0.011274814      0.000062   \n\n     xopt_error  \n1         False  \n2         False  \n3         False  \n4         False  \n5         False  \n..          ...  \n201       False  \n202       False  \n203       False  \n204       False  \n205       False  \n\n[205 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0</th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>x4</th>\n      <th>x5</th>\n      <th>x6</th>\n      <th>x7</th>\n      <th>x8</th>\n      <th>x9</th>\n      <th>...</th>\n      <th>x13</th>\n      <th>x14</th>\n      <th>x15</th>\n      <th>x16</th>\n      <th>x17</th>\n      <th>x18</th>\n      <th>x19</th>\n      <th>f</th>\n      <th>xopt_runtime</th>\n      <th>xopt_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.062995</td>\n      <td>-0.062279</td>\n      <td>0.544839</td>\n      <td>-0.503431</td>\n      <td>0.821776</td>\n      <td>0.629965</td>\n      <td>-0.474384</td>\n      <td>-0.033765</td>\n      <td>-0.387065</td>\n      <td>-0.940502</td>\n      <td>...</td>\n      <td>-0.615613</td>\n      <td>-0.160023</td>\n      <td>-0.488284</td>\n      <td>-0.947566</td>\n      <td>0.328403</td>\n      <td>-0.366973</td>\n      <td>0.738352</td>\n      <td>6.1677294</td>\n      <td>0.000156</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.561243</td>\n      <td>0.512332</td>\n      <td>-0.632823</td>\n      <td>0.350470</td>\n      <td>-0.523234</td>\n      <td>0.384745</td>\n      <td>-0.785675</td>\n      <td>-0.149247</td>\n      <td>-0.831706</td>\n      <td>-0.579351</td>\n      <td>...</td>\n      <td>-0.314386</td>\n      <td>-0.732740</td>\n      <td>0.657981</td>\n      <td>-0.765338</td>\n      <td>0.152458</td>\n      <td>0.349693</td>\n      <td>0.601206</td>\n      <td>6.9899263</td>\n      <td>0.000030</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.901420</td>\n      <td>-0.238834</td>\n      <td>0.060313</td>\n      <td>0.320931</td>\n      <td>0.515013</td>\n      <td>-0.946428</td>\n      <td>0.467778</td>\n      <td>0.294698</td>\n      <td>-0.681843</td>\n      <td>0.671658</td>\n      <td>...</td>\n      <td>0.524845</td>\n      <td>0.225160</td>\n      <td>0.236900</td>\n      <td>-0.977615</td>\n      <td>0.674749</td>\n      <td>-0.572095</td>\n      <td>-0.380031</td>\n      <td>6.0905423</td>\n      <td>0.000025</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.368522</td>\n      <td>-0.159474</td>\n      <td>0.759957</td>\n      <td>0.700507</td>\n      <td>-0.905647</td>\n      <td>0.287221</td>\n      <td>-0.391568</td>\n      <td>-0.171503</td>\n      <td>-0.177241</td>\n      <td>0.405613</td>\n      <td>...</td>\n      <td>-0.494040</td>\n      <td>0.322425</td>\n      <td>-0.006682</td>\n      <td>0.390137</td>\n      <td>-0.551452</td>\n      <td>-0.388566</td>\n      <td>0.240009</td>\n      <td>5.3742104</td>\n      <td>0.000034</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.551577</td>\n      <td>-0.984087</td>\n      <td>-0.566301</td>\n      <td>0.200010</td>\n      <td>0.372020</td>\n      <td>-0.720478</td>\n      <td>-0.538366</td>\n      <td>-0.699903</td>\n      <td>-0.799647</td>\n      <td>-0.825138</td>\n      <td>...</td>\n      <td>-0.529741</td>\n      <td>-0.016578</td>\n      <td>0.406307</td>\n      <td>0.833492</td>\n      <td>0.959548</td>\n      <td>-0.175618</td>\n      <td>-0.134072</td>\n      <td>7.0636697</td>\n      <td>0.000024</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>-0.027026</td>\n      <td>0.035315</td>\n      <td>-0.030245</td>\n      <td>0.030285</td>\n      <td>0.020834</td>\n      <td>-0.016995</td>\n      <td>0.009642</td>\n      <td>0.018554</td>\n      <td>0.019674</td>\n      <td>-0.018592</td>\n      <td>...</td>\n      <td>-0.021082</td>\n      <td>0.028225</td>\n      <td>-0.024120</td>\n      <td>-0.003468</td>\n      <td>-0.010130</td>\n      <td>0.020217</td>\n      <td>-0.041383</td>\n      <td>0.012211022</td>\n      <td>0.000061</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>0.006212</td>\n      <td>0.035013</td>\n      <td>0.012527</td>\n      <td>-0.003776</td>\n      <td>0.022976</td>\n      <td>0.016143</td>\n      <td>-0.014786</td>\n      <td>0.018250</td>\n      <td>-0.009620</td>\n      <td>0.010238</td>\n      <td>...</td>\n      <td>0.002984</td>\n      <td>-0.004988</td>\n      <td>-0.037240</td>\n      <td>-0.003862</td>\n      <td>-0.016839</td>\n      <td>-0.001926</td>\n      <td>-0.041088</td>\n      <td>0.008816749</td>\n      <td>0.000055</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>-0.026574</td>\n      <td>0.034857</td>\n      <td>0.012320</td>\n      <td>0.003896</td>\n      <td>0.022817</td>\n      <td>0.015985</td>\n      <td>-0.023450</td>\n      <td>-0.013961</td>\n      <td>0.019353</td>\n      <td>0.010468</td>\n      <td>...</td>\n      <td>-0.029400</td>\n      <td>0.027764</td>\n      <td>-0.024597</td>\n      <td>-0.004058</td>\n      <td>-0.011925</td>\n      <td>0.025997</td>\n      <td>-0.009412</td>\n      <td>0.0097027</td>\n      <td>0.000047</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>-0.017086</td>\n      <td>0.034640</td>\n      <td>-0.022766</td>\n      <td>0.029541</td>\n      <td>-0.007904</td>\n      <td>0.015767</td>\n      <td>-0.017971</td>\n      <td>0.017889</td>\n      <td>0.019539</td>\n      <td>0.012732</td>\n      <td>...</td>\n      <td>0.002497</td>\n      <td>-0.004412</td>\n      <td>-0.043894</td>\n      <td>-0.004313</td>\n      <td>-0.026123</td>\n      <td>-0.003168</td>\n      <td>-0.040722</td>\n      <td>0.010275573</td>\n      <td>0.000067</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>-0.017817</td>\n      <td>0.034497</td>\n      <td>0.011844</td>\n      <td>0.016088</td>\n      <td>0.022448</td>\n      <td>0.015622</td>\n      <td>0.011351</td>\n      <td>0.017745</td>\n      <td>-0.003923</td>\n      <td>0.001219</td>\n      <td>...</td>\n      <td>0.002310</td>\n      <td>0.027400</td>\n      <td>-0.027870</td>\n      <td>-0.004496</td>\n      <td>-0.031345</td>\n      <td>-0.003008</td>\n      <td>-0.009764</td>\n      <td>0.011274814</td>\n      <td>0.000062</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>205 rows Ã— 23 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the collected data\n",
    "X.data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the trust region"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0262,  0.0031, -0.0092,  0.0048, -0.0669, -0.0380, -0.0045, -0.0580,\n         -0.0433, -0.0313, -0.0030, -0.0406, -0.0292, -0.0051, -0.0098, -0.0162,\n         -0.0230, -0.0136, -0.0090, -0.0175],\n        [ 0.0057,  0.0345,  0.0256,  0.0382, -0.0311,  0.0023,  0.0274, -0.0250,\n         -0.0045,  0.0010,  0.0321, -0.0098,  0.0118,  0.0294,  0.0224,  0.0156,\n          0.0122,  0.0177,  0.0301,  0.0247]], dtype=torch.float64)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.generator.get_trust_region()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customizing optimization\n",
    "Each generator has a set of options that can be modified to effect optimization behavior"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'optim': {'num_restarts': 20,\n  'raw_samples': 20,\n  'sequential': True,\n  'use_nearby_initial_points': False,\n  'max_travel_distances': None},\n 'acq': {'proximal_lengthscales': None,\n  'use_transformed_proximal_weights': True,\n  'monte_carlo_samples': 128,\n  'beta': 2.0},\n 'model': {'name': 'standard',\n  'custom_constructor': None,\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {}},\n 'n_initial': 5,\n 'use_cuda': False}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.generator.options.dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# example: add a Gamma(1.0,10.0) prior to the noise hyperparameter to reduce model noise\n",
    "# (good for optimizing noise-free simulations)\n",
    "X.generator.options.model.use_low_noise_prior = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-18.9156], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(2.3018, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(-1.5870, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.2928, 0.2690, 0.4275, 0.3611, 0.4694, 0.6586, 0.2928, 0.3429, 0.5969,\n           0.3127, 0.4361, 0.2421, 0.6849, 0.4104, 0.3078, 0.2881, 0.4446, 0.2677,\n           0.6057, 0.7308]], dtype=torch.float64, requires_grad=True))]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X.generator.model.named_parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
