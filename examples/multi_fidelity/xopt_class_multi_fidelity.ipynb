{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-fidelity (MF) optimization\n",
    "\n",
    "In most cases it is better to do many cheap evaluations of an approximation to the target function than it is to only optimize the target function. This example demonstrates the 'multi-fidelity' capabilities of xopt. \n",
    "\n",
    "We follow the implementation of multi-fidelity bayesian optimization used in botorch https://botorch.org/tutorials/multi_fidelity_bo to optimize the synthetic test function AugmentedHartmann https://botorch.org/api/test_functions.html.\n",
    "\n",
    "The difference between normal Bayesian optimization and MF optimization is that we specify a 'cost' to making observations at a given fidelity. For this example we assume a base cost of 5 and a fidelity cost between 0-1. The algorithm should make many observations at lower fidelity relative to higher fidelity, lowering the total observation cost. \n",
    "\n",
    "NOTE: The cost parameter is required to be the LAST element of the variables list. Also this method is best suited for parallel observations of the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the class\n",
    "from xopt import Xopt\n",
    "from xopt.bayesian.generators.multi_fidelity import MultiFidelityGenerator\n",
    "from xopt.bayesian.models.models import create_multi_fidelity_model\n",
    "from botorch.test_functions.multi_fidelity import AugmentedHartmann\n",
    "import logging\n",
    "import os\n",
    "SMOKE_TEST = os.environ.get('SMOKE_TEST')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from botorch.acquisition.analytic import UpperConfidenceBound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Xopt` object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.\n",
    "\n",
    "Here we will make one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a proper input file. \n",
    "import yaml\n",
    "YAML = \"\"\"\n",
    "xopt: {output_path: null, verbose: true}\n",
    "\n",
    "algorithm:\n",
    "  name: multi_fidelity\n",
    "  options:  \n",
    "      processes: 4\n",
    "      budget: 32\n",
    "      verbose: True\n",
    "      generator_options: {}\n",
    "\n",
    "simulation: \n",
    "  name: test_multi_fidelity\n",
    "  evaluate: xopt.evaluators.test_multi_fidelity.evaluate\n",
    "\n",
    "vocs:\n",
    "  name: test_multi_fidelity\n",
    "  description: null\n",
    "  simulation: test_multi_fidelity\n",
    "  templates: null\n",
    "  variables:\n",
    "    x1: [0, 1.0]\n",
    "    x2: [0, 1.0]\n",
    "    x3: [0, 1.0]\n",
    "    x4: [0, 1.0]\n",
    "    x5: [0, 1.0]\n",
    "    x6: [0, 1.0]\n",
    "    cost: [0, 1.0]                          ## NOTE: THIS IS REQUIRED FOR MULTI-FIDELITY OPTIMIZATION\n",
    "  objectives:\n",
    "    y1: 'MINIMIZE'\n",
    "  linked_variables: {}\n",
    "  constants: {a: dummy_constant}\n",
    "\n",
    "\"\"\"\n",
    "config = yaml.safe_load(YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config as dict.\n",
      "Warning: No path set for key xopt : output_path\n",
      "Warning: No path set for key algorithm : options : restart_file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "            Xopt \n",
       "________________________________           \n",
       "Version: 0.4.3+137.g5ac4672.dirty\n",
       "Configured: True\n",
       "Config as YAML:\n",
       "xopt: {output_path: null, verbose: true, algorithm: cnsga}\n",
       "algorithm:\n",
       "  name: multi_fidelity\n",
       "  function: xopt.bayesian.algorithms.multi_fidelity_optimize\n",
       "  options:\n",
       "    processes: 4\n",
       "    budget: 32\n",
       "    verbose: true\n",
       "    generator_options: {}\n",
       "    base_cost: 1.0\n",
       "    custom_model: !!python/name:xopt.bayesian.models.models.create_multi_fidelity_model ''\n",
       "    restart_file: null\n",
       "    initial_x: null\n",
       "simulation:\n",
       "  name: test_multi_fidelity\n",
       "  evaluate: xopt.evaluators.test_multi_fidelity.evaluate\n",
       "  options: {extra_option: abc}\n",
       "vocs:\n",
       "  name: test_multi_fidelity\n",
       "  description: null\n",
       "  simulation: test_multi_fidelity\n",
       "  templates: null\n",
       "  variables:\n",
       "    x1: [0, 1.0]\n",
       "    x2: [0, 1.0]\n",
       "    x3: [0, 1.0]\n",
       "    x4: [0, 1.0]\n",
       "    x5: [0, 1.0]\n",
       "    x6: [0, 1.0]\n",
       "    cost: [0, 1.0]\n",
       "  objectives: {y1: MINIMIZE}\n",
       "  linked_variables: {}\n",
       "  constants: {a: dummy_constant}\n",
       "  constraints: {}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    config['algorithm']['options']['budget'] = 3\n",
    "    config['algorithm']['options']['processes'] = 1\n",
    "    config['algorithm']['options']['generator_options']['num_restarts'] = 2\n",
    "    config['algorithm']['options']['generator_options']['raw_samples'] = 2\n",
    "    config['algorithm']['options']['generator_options']['base_acq'] = None\n",
    "\n",
    "X = Xopt(config)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of these\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "#from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "#executor = PoolExecutor()\n",
    "# This will also work. \n",
    "executor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xopt.bayesian.asynch_optimize:started running optimization with generator: <xopt.bayesian.generators.multi_fidelity.MultiFidelityGenerator object at 0x000001E0CFA1D0D0>\n",
      "INFO:xopt.bayesian.asynch_optimize:starting optimization loop\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 0: tensor([0.7905, 0.9636, 0.5610, 0.2393, 0.6601, 0.0253, 0.2710],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 1.2709610164165497\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 1: tensor([0.5729, 0.1048, 0.7607, 0.9095, 0.0789, 0.5335, 0.4867],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 2.7576792538166046\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 2: tensor([0.5447, 0.8570, 0.8708, 0.6038, 0.6233, 0.6443, 0.4854],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 4.243029624223709\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 3: tensor([0.2601, 0.8064, 0.7882, 0.1635, 0.7716, 0.7865, 0.3575],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 5.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:generating 4 new candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at time 2021-09-13T16:41:31-05:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xopt.bayesian.asynch_optimize:Model creation time: 0.106 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate generation time: 23.95 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate(s): tensor([[0.4086, 0.8321, 0.7236, 0.0122, 0.7440, 0.5600, 0.0000],\n",
      "        [0.3151, 0.7444, 0.6419, 0.0435, 0.8621, 0.9829, 0.0000],\n",
      "        [0.0365, 0.6222, 0.7808, 0.0951, 0.6517, 0.7422, 0.0000],\n",
      "        [0.0846, 0.9584, 0.8682, 0.0943, 0.9642, 0.7611, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 4: tensor([0.4086, 0.8321, 0.7236, 0.0122, 0.7440, 0.5600, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 6.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 5: tensor([0.3151, 0.7444, 0.6419, 0.0435, 0.8621, 0.9829, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 7.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 6: tensor([0.0365, 0.6222, 0.7808, 0.0951, 0.6517, 0.7422, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 8.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 7: tensor([0.0846, 0.9584, 0.8682, 0.0943, 0.9642, 0.7611, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 9.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:generating 4 new candidates\n",
      "INFO:xopt.bayesian.asynch_optimize:Model creation time: 0.06252 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate generation time: 25.8 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate(s): tensor([[0.0075, 0.5422, 0.5915, 0.0821, 0.6587, 0.7026, 0.0000],\n",
      "        [0.0124, 0.6033, 0.7385, 0.0036, 0.4977, 0.8040, 0.0000],\n",
      "        [0.0000, 0.5624, 0.7919, 0.2597, 0.5645, 0.6786, 0.0000],\n",
      "        [0.0420, 0.4724, 0.8907, 0.0795, 0.6461, 0.7582, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 8: tensor([0.0075, 0.5422, 0.5915, 0.0821, 0.6587, 0.7026, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 10.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 9: tensor([0.0124, 0.6033, 0.7385, 0.0036, 0.4977, 0.8040, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 11.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 10: tensor([0.0000, 0.5624, 0.7919, 0.2597, 0.5645, 0.6786, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 12.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 11: tensor([0.0420, 0.4724, 0.8907, 0.0795, 0.6461, 0.7582, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 13.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:generating 4 new candidates\n",
      "INFO:xopt.bayesian.asynch_optimize:Model creation time: 0.1091 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate generation time: 26.47 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate(s): tensor([[4.0553e-04, 4.8673e-01, 4.8793e-01, 0.0000e+00, 4.8113e-01, 7.8099e-01,\n",
      "         0.0000e+00],\n",
      "        [1.9654e-03, 3.9234e-01, 5.9229e-01, 3.0821e-01, 5.3242e-01, 7.7564e-01,\n",
      "         0.0000e+00],\n",
      "        [2.4241e-03, 4.8774e-01, 6.1071e-01, 1.1916e-01, 4.8540e-01, 4.7798e-01,\n",
      "         0.0000e+00],\n",
      "        [6.1647e-04, 6.7837e-01, 5.4033e-01, 2.9968e-01, 4.9982e-01, 6.8235e-01,\n",
      "         0.0000e+00]], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 12: tensor([4.0553e-04, 4.8673e-01, 4.8793e-01, 0.0000e+00, 4.8113e-01, 7.8099e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 14.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 13: tensor([0.0020, 0.3923, 0.5923, 0.3082, 0.5324, 0.7756, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 15.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 14: tensor([0.0024, 0.4877, 0.6107, 0.1192, 0.4854, 0.4780, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 16.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 15: tensor([6.1647e-04, 6.7837e-01, 5.4033e-01, 2.9968e-01, 4.9982e-01, 6.8235e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 17.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:generating 4 new candidates\n",
      "INFO:xopt.bayesian.asynch_optimize:Model creation time: 0.218 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate generation time: 27.28 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate(s): tensor([[6.9370e-04, 3.4325e-01, 5.3227e-01, 3.8780e-01, 5.0984e-01, 5.7876e-01,\n",
      "         0.0000e+00],\n",
      "        [2.4448e-04, 3.0891e-01, 5.0302e-01, 3.9287e-01, 4.9149e-01, 8.5210e-01,\n",
      "         0.0000e+00],\n",
      "        [3.9543e-04, 2.4311e-01, 5.8282e-01, 2.7193e-01, 4.9124e-01, 6.9098e-01,\n",
      "         0.0000e+00],\n",
      "        [6.6739e-04, 3.5537e-01, 5.8327e-01, 3.4999e-01, 4.0010e-01, 7.1351e-01,\n",
      "         0.0000e+00]], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 16: tensor([0.0007, 0.3433, 0.5323, 0.3878, 0.5098, 0.5788, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 18.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 17: tensor([2.4448e-04, 3.0891e-01, 5.0302e-01, 3.9287e-01, 4.9149e-01, 8.5210e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 19.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 18: tensor([3.9543e-04, 2.4311e-01, 5.8282e-01, 2.7193e-01, 4.9124e-01, 6.9098e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 20.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 19: tensor([6.6739e-04, 3.5537e-01, 5.8327e-01, 3.4999e-01, 4.0010e-01, 7.1351e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 21.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:generating 4 new candidates\n",
      "INFO:xopt.bayesian.asynch_optimize:Model creation time: 0.275 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate generation time: 29.15 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate(s): tensor([[1.6412e-04, 2.3406e-01, 6.7942e-01, 2.8909e-01, 3.5134e-01, 7.2709e-01,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.9815e-01, 5.5475e-01, 4.1225e-01, 3.6133e-01, 6.9962e-01,\n",
      "         0.0000e+00],\n",
      "        [5.9029e-05, 2.9058e-01, 6.4939e-01, 3.4364e-01, 3.4654e-01, 6.3518e-01,\n",
      "         0.0000e+00],\n",
      "        [6.6183e-05, 2.6533e-01, 5.0161e-01, 2.6819e-01, 3.4608e-01, 6.8546e-01,\n",
      "         0.0000e+00]], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 20: tensor([1.6412e-04, 2.3406e-01, 6.7942e-01, 2.8909e-01, 3.5134e-01, 7.2709e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 22.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 21: tensor([0.0000, 0.1982, 0.5547, 0.4123, 0.3613, 0.6996, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 23.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 22: tensor([5.9029e-05, 2.9058e-01, 6.4939e-01, 3.4364e-01, 3.4654e-01, 6.3518e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 24.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 23: tensor([6.6183e-05, 2.6533e-01, 5.0161e-01, 2.6819e-01, 3.4608e-01, 6.8546e-01,\n",
      "        0.0000e+00], dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 25.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:generating 4 new candidates\n",
      "INFO:xopt.bayesian.asynch_optimize:Model creation time: 0.302 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate generation time: 31.33 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate(s): tensor([[0.0061, 0.2475, 0.4473, 0.2715, 0.2369, 0.6721, 0.0000],\n",
      "        [0.0381, 0.1456, 0.4370, 0.2648, 0.2835, 0.7451, 0.0000],\n",
      "        [0.1265, 0.2072, 0.4633, 0.2676, 0.2960, 0.6658, 0.0000],\n",
      "        [0.0151, 0.1136, 0.4469, 0.2476, 0.2924, 0.6254, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 24: tensor([0.0061, 0.2475, 0.4473, 0.2715, 0.2369, 0.6721, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 26.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 25: tensor([0.0381, 0.1456, 0.4370, 0.2648, 0.2835, 0.7451, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 27.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 26: tensor([0.1265, 0.2072, 0.4633, 0.2676, 0.2960, 0.6658, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 28.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 27: tensor([0.0151, 0.1136, 0.4469, 0.2476, 0.2924, 0.6254, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 29.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:generating 4 new candidates\n",
      "INFO:xopt.bayesian.asynch_optimize:Model creation time: 0.759 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate generation time: 35.49 s\n",
      "INFO:xopt.bayesian.asynch_optimize:Candidate(s): tensor([[0.2224, 0.0928, 0.5355, 0.2754, 0.2921, 0.6336, 0.0000],\n",
      "        [0.3854, 0.1156, 0.0826, 0.9140, 0.5339, 0.4290, 0.0000],\n",
      "        [0.2083, 0.1609, 0.4295, 0.2047, 0.2985, 0.6464, 0.0000],\n",
      "        [0.1965, 0.1529, 0.3587, 0.2914, 0.3053, 0.6172, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 28: tensor([0.2224, 0.0928, 0.5355, 0.2754, 0.2921, 0.6336, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 30.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 29: tensor([0.3854, 0.1156, 0.0826, 0.9140, 0.5339, 0.4290, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 31.600572168827057\n",
      "INFO:xopt.bayesian.asynch_optimize:Submitting candidate 30: tensor([0.2083, 0.1609, 0.4295, 0.2047, 0.2985, 0.6464, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "INFO:xopt.bayesian.asynch_optimize:total cost: 32.60057216882706\n",
      "INFO:xopt.bayesian.asynch_optimize:budget exceeded, waiting for simulations to end\n",
      "INFO:xopt.bayesian.asynch_optimize:Budget exceeded and simulations finished\n"
     ]
    }
   ],
   "source": [
    "# Change max generations\n",
    "X.run(executor=executor)\n",
    "results = X.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get highest fidelity global optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator object\n",
    "gen = MultiFidelityGenerator(X.vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = create_multi_fidelity_model(results['variables'], results['corrected_objectives'], results['corrected_constraints'], X.vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.3122], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE: we want to get the minimum evaluated at the highest fidelity -> make sure to use get_recommendation\n",
    "rec = gen.get_recommendation(model)\n",
    "problem = AugmentedHartmann(negate=False)\n",
    "problem(rec) ## NOTE: the correct global minimum is -3.32237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xopt",
   "language": "python",
   "name": "xopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
