{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking TuRBO Bayesian Optimization\n",
    "In this tutorial we demonstrate the use of Xopt to preform Bayesian Optimization on\n",
    "the 20D Ackley test function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the test problem\n",
    "Here we define a simple optimization problem, where we attempt to minimize the sin\n",
    "function in the domian [0,2*pi]. Note that the function used to evaluate the\n",
    "objective function takes a dictionary as input and returns a dictionary as the output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from xopt.resources.test_functions.sphere_20 import vocs, evaluate_sphere"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Xopt objects\n",
    "Create the evaluator to evaluate our test function and create a generator that uses\n",
    "the Upper Confidence Bound acqusition function to perform Bayesian Optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from xopt.evaluator import Evaluator\n",
    "from xopt.generators.bayesian import UpperConfidenceBoundGenerator\n",
    "from xopt import Xopt\n",
    "\n",
    "evaluator = Evaluator(function=evaluate_sphere)\n",
    "options = UpperConfidenceBoundGenerator.default_options()\n",
    "options.n_initial = 5\n",
    "options.optim.use_turbo = True\n",
    "generator = UpperConfidenceBoundGenerator(vocs, options)\n",
    "\n",
    "X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate and evaluate initial points\n",
    "To begin optimization, we must generate some random initial data points. The first call\n",
    "to `X.step()` will generate and evaluate a number of randomly points specified by the\n",
    " generator. Note that if we add data to xopt before calling `X.step()` by assigning\n",
    " the data to `X.data`, calls to `X.step()` will ignore the random generation and\n",
    " proceed to generating points via Bayesian optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "         x0        x1        x2        x3        x4        x5        x6  \\\n1  0.422018 -0.026224  0.718189  0.310911  0.707492  0.434676 -0.849791   \n2 -0.882145 -0.562799  0.733046  0.649420  0.283789  0.704892  0.618188   \n3  0.721131  0.314384 -0.289785 -0.852108 -0.844845 -0.384828  0.813833   \n4  0.980294 -0.875082  0.657125 -0.737197  0.140548 -0.459261  0.165984   \n5 -0.279773 -0.499483  0.918083  0.838570 -0.271987  0.706489 -0.546202   \n\n         x7        x8        x9  ...       x13       x14       x15       x16  \\\n1  0.970433 -0.413621  0.716674  ...  0.824777 -0.944750 -0.944922 -0.199756   \n2 -0.841374  0.417714 -0.108155  ...  0.325305 -0.264091  0.458591  0.101729   \n3  0.312714  0.713930 -0.797754  ... -0.379077 -0.007176  0.454549  0.444502   \n4 -0.273056  0.499388 -0.598485  ... -0.289586 -0.020493  0.948895 -0.658809   \n5  0.324510  0.836784  0.985140  ... -0.323156 -0.645561 -0.708913 -0.155306   \n\n        x17       x18       x19          f  xopt_runtime  xopt_error  \n1 -0.873053  0.873657  0.466680   9.041967      0.000166       False  \n2  0.163075 -0.542988 -0.334861   5.862751      0.000035       False  \n3 -0.237513  0.553099 -0.925864   7.220641      0.000025       False  \n4  0.005012  0.604305 -0.185447   5.603789      0.000025       False  \n5 -0.549053 -0.482431  0.728909  7.1246204      0.000024       False  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0</th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>x4</th>\n      <th>x5</th>\n      <th>x6</th>\n      <th>x7</th>\n      <th>x8</th>\n      <th>x9</th>\n      <th>...</th>\n      <th>x13</th>\n      <th>x14</th>\n      <th>x15</th>\n      <th>x16</th>\n      <th>x17</th>\n      <th>x18</th>\n      <th>x19</th>\n      <th>f</th>\n      <th>xopt_runtime</th>\n      <th>xopt_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.422018</td>\n      <td>-0.026224</td>\n      <td>0.718189</td>\n      <td>0.310911</td>\n      <td>0.707492</td>\n      <td>0.434676</td>\n      <td>-0.849791</td>\n      <td>0.970433</td>\n      <td>-0.413621</td>\n      <td>0.716674</td>\n      <td>...</td>\n      <td>0.824777</td>\n      <td>-0.944750</td>\n      <td>-0.944922</td>\n      <td>-0.199756</td>\n      <td>-0.873053</td>\n      <td>0.873657</td>\n      <td>0.466680</td>\n      <td>9.041967</td>\n      <td>0.000166</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.882145</td>\n      <td>-0.562799</td>\n      <td>0.733046</td>\n      <td>0.649420</td>\n      <td>0.283789</td>\n      <td>0.704892</td>\n      <td>0.618188</td>\n      <td>-0.841374</td>\n      <td>0.417714</td>\n      <td>-0.108155</td>\n      <td>...</td>\n      <td>0.325305</td>\n      <td>-0.264091</td>\n      <td>0.458591</td>\n      <td>0.101729</td>\n      <td>0.163075</td>\n      <td>-0.542988</td>\n      <td>-0.334861</td>\n      <td>5.862751</td>\n      <td>0.000035</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.721131</td>\n      <td>0.314384</td>\n      <td>-0.289785</td>\n      <td>-0.852108</td>\n      <td>-0.844845</td>\n      <td>-0.384828</td>\n      <td>0.813833</td>\n      <td>0.312714</td>\n      <td>0.713930</td>\n      <td>-0.797754</td>\n      <td>...</td>\n      <td>-0.379077</td>\n      <td>-0.007176</td>\n      <td>0.454549</td>\n      <td>0.444502</td>\n      <td>-0.237513</td>\n      <td>0.553099</td>\n      <td>-0.925864</td>\n      <td>7.220641</td>\n      <td>0.000025</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.980294</td>\n      <td>-0.875082</td>\n      <td>0.657125</td>\n      <td>-0.737197</td>\n      <td>0.140548</td>\n      <td>-0.459261</td>\n      <td>0.165984</td>\n      <td>-0.273056</td>\n      <td>0.499388</td>\n      <td>-0.598485</td>\n      <td>...</td>\n      <td>-0.289586</td>\n      <td>-0.020493</td>\n      <td>0.948895</td>\n      <td>-0.658809</td>\n      <td>0.005012</td>\n      <td>0.604305</td>\n      <td>-0.185447</td>\n      <td>5.603789</td>\n      <td>0.000025</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.279773</td>\n      <td>-0.499483</td>\n      <td>0.918083</td>\n      <td>0.838570</td>\n      <td>-0.271987</td>\n      <td>0.706489</td>\n      <td>-0.546202</td>\n      <td>0.324510</td>\n      <td>0.836784</td>\n      <td>0.985140</td>\n      <td>...</td>\n      <td>-0.323156</td>\n      <td>-0.645561</td>\n      <td>-0.708913</td>\n      <td>-0.155306</td>\n      <td>-0.549053</td>\n      <td>-0.482431</td>\n      <td>0.728909</td>\n      <td>7.1246204</td>\n      <td>0.000024</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 23 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate random initial points\n",
    "X.step()\n",
    "\n",
    "# inspect the gathered data\n",
    "X.data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n         -1., -1., -1., -1., -1., -1.],\n        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n          1.,  1.,  1.,  1.,  1.,  1.]], dtype=torch.float64)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine trust region from gathered data\n",
    "generator.train_model()\n",
    "generator.trust_region"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Do bayesian optimization steps\n",
    "To perform optimization we simply call `X.step()` in a loop. This allows us to do\n",
    "intermediate tasks in between optimization steps, such as examining the model and\n",
    "acquisition function at each step (as we demonstrate here)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0: length:0.5, sc:1, fc:0,best_val: 7.12462043762207\n",
      "1: length:0.5, sc:2, fc:0,best_val: 7.12462043762207\n",
      "2: length:0.5, sc:3, fc:0,best_val: 6.596367835998535\n",
      "3: length:0.5, sc:4, fc:0,best_val: 3.1890828609466553\n",
      "4: length:0.5, sc:0, fc:1,best_val: 3.1890828609466553\n",
      "5: length:0.5, sc:0, fc:2,best_val: 3.1890828609466553\n",
      "6: length:0.5, sc:0, fc:3,best_val: 3.1890828609466553\n",
      "7: length:0.5, sc:0, fc:4,best_val: 3.1890828609466553\n",
      "8: length:0.5, sc:0, fc:5,best_val: 3.1890828609466553\n",
      "9: length:0.5, sc:0, fc:6,best_val: 3.1890828609466553\n",
      "10: length:0.5, sc:0, fc:7,best_val: 3.1890828609466553\n",
      "11: length:0.5, sc:1, fc:0,best_val: 2.817741870880127\n",
      "12: length:0.5, sc:0, fc:1,best_val: 2.817741870880127\n",
      "13: length:0.5, sc:0, fc:2,best_val: 2.817741870880127\n",
      "14: length:0.5, sc:0, fc:3,best_val: 2.817741870880127\n",
      "15: length:0.5, sc:0, fc:4,best_val: 2.817741870880127\n",
      "16: length:0.5, sc:0, fc:5,best_val: 2.817741870880127\n",
      "17: length:0.5, sc:0, fc:6,best_val: 2.817741870880127\n",
      "18: length:0.5, sc:0, fc:7,best_val: 2.817741870880127\n",
      "19: length:0.5, sc:0, fc:8,best_val: 2.817741870880127\n",
      "20: length:0.5, sc:0, fc:9,best_val: 2.817741870880127\n",
      "21: length:0.5, sc:0, fc:10,best_val: 2.817741870880127\n",
      "22: length:0.5, sc:0, fc:11,best_val: 2.817741870880127\n",
      "23: length:0.5, sc:0, fc:12,best_val: 2.817741870880127\n",
      "24: length:0.5, sc:0, fc:13,best_val: 2.817741870880127\n",
      "25: length:0.5, sc:0, fc:14,best_val: 2.817741870880127\n",
      "26: length:0.5, sc:0, fc:15,best_val: 2.817741870880127\n",
      "27: length:0.5, sc:0, fc:16,best_val: 2.817741870880127\n",
      "28: length:0.5, sc:0, fc:17,best_val: 2.817741870880127\n",
      "29: length:0.5, sc:0, fc:18,best_val: 2.817741870880127\n",
      "30: length:0.5, sc:0, fc:19,best_val: 2.817741870880127\n",
      "31: length:0.25, sc:0, fc:0,best_val: 2.817741870880127\n",
      "32: length:0.25, sc:0, fc:1,best_val: 2.817741870880127\n",
      "33: length:0.25, sc:0, fc:2,best_val: 2.817741870880127\n",
      "34: length:0.25, sc:0, fc:3,best_val: 2.817741870880127\n",
      "35: length:0.25, sc:0, fc:4,best_val: 2.817741870880127\n",
      "36: length:0.25, sc:0, fc:5,best_val: 2.817741870880127\n",
      "37: length:0.25, sc:0, fc:6,best_val: 2.817741870880127\n",
      "38: length:0.25, sc:0, fc:7,best_val: 2.817741870880127\n",
      "39: length:0.25, sc:0, fc:8,best_val: 2.817741870880127\n",
      "40: length:0.25, sc:0, fc:9,best_val: 2.817741870880127\n",
      "41: length:0.25, sc:0, fc:10,best_val: 2.817741870880127\n",
      "42: length:0.25, sc:0, fc:11,best_val: 2.817741870880127\n",
      "43: length:0.25, sc:0, fc:12,best_val: 2.817741870880127\n",
      "44: length:0.25, sc:0, fc:13,best_val: 2.817741870880127\n",
      "45: length:0.25, sc:0, fc:14,best_val: 2.817741870880127\n",
      "46: length:0.25, sc:0, fc:15,best_val: 2.817741870880127\n",
      "47: length:0.25, sc:0, fc:16,best_val: 2.817741870880127\n",
      "48: length:0.25, sc:0, fc:17,best_val: 2.817741870880127\n",
      "49: length:0.25, sc:0, fc:18,best_val: 2.817741870880127\n",
      "50: length:0.25, sc:0, fc:19,best_val: 2.817741870880127\n",
      "51: length:0.125, sc:0, fc:0,best_val: 2.817741870880127\n",
      "52: length:0.125, sc:0, fc:1,best_val: 2.817741870880127\n",
      "53: length:0.125, sc:0, fc:2,best_val: 2.817741870880127\n",
      "54: length:0.125, sc:1, fc:0,best_val: 2.653550386428833\n",
      "55: length:0.125, sc:0, fc:1,best_val: 2.653550386428833\n",
      "56: length:0.125, sc:0, fc:2,best_val: 2.653550386428833\n",
      "57: length:0.125, sc:0, fc:3,best_val: 2.653550386428833\n",
      "58: length:0.125, sc:0, fc:4,best_val: 2.653550386428833\n",
      "59: length:0.125, sc:0, fc:5,best_val: 2.653550386428833\n",
      "60: length:0.125, sc:0, fc:6,best_val: 2.653550386428833\n",
      "61: length:0.125, sc:0, fc:7,best_val: 2.653550386428833\n",
      "62: length:0.125, sc:0, fc:8,best_val: 2.653550386428833\n",
      "63: length:0.125, sc:0, fc:9,best_val: 2.653550386428833\n",
      "64: length:0.125, sc:0, fc:10,best_val: 2.653550386428833\n",
      "65: length:0.125, sc:0, fc:11,best_val: 2.653550386428833\n",
      "66: length:0.125, sc:0, fc:12,best_val: 2.653550386428833\n",
      "67: length:0.125, sc:0, fc:13,best_val: 2.653550386428833\n",
      "68: length:0.125, sc:0, fc:14,best_val: 2.653550386428833\n",
      "69: length:0.125, sc:0, fc:15,best_val: 2.653550386428833\n",
      "70: length:0.125, sc:0, fc:16,best_val: 2.653550386428833\n",
      "71: length:0.125, sc:0, fc:17,best_val: 2.653550386428833\n",
      "72: length:0.125, sc:0, fc:18,best_val: 2.653550386428833\n",
      "73: length:0.125, sc:0, fc:19,best_val: 2.653550386428833\n",
      "74: length:0.0625, sc:0, fc:0,best_val: 2.653550386428833\n",
      "75: length:0.0625, sc:0, fc:1,best_val: 2.653550386428833\n",
      "76: length:0.0625, sc:0, fc:2,best_val: 2.653550386428833\n",
      "77: length:0.0625, sc:0, fc:3,best_val: 2.653550386428833\n",
      "78: length:0.0625, sc:0, fc:4,best_val: 2.653550386428833\n",
      "79: length:0.0625, sc:0, fc:5,best_val: 2.653550386428833\n",
      "80: length:0.0625, sc:0, fc:6,best_val: 2.653550386428833\n",
      "81: length:0.0625, sc:1, fc:0,best_val: 2.2709734439849854\n",
      "82: length:0.0625, sc:0, fc:1,best_val: 2.2709734439849854\n",
      "83: length:0.0625, sc:0, fc:2,best_val: 2.2709734439849854\n",
      "84: length:0.0625, sc:0, fc:3,best_val: 2.2709734439849854\n",
      "85: length:0.0625, sc:0, fc:4,best_val: 2.2709734439849854\n",
      "86: length:0.0625, sc:0, fc:5,best_val: 2.2709734439849854\n",
      "87: length:0.0625, sc:0, fc:6,best_val: 2.2709734439849854\n",
      "88: length:0.0625, sc:0, fc:7,best_val: 2.2709734439849854\n",
      "89: length:0.0625, sc:0, fc:8,best_val: 2.2709734439849854\n",
      "90: length:0.0625, sc:0, fc:9,best_val: 2.2709734439849854\n",
      "91: length:0.0625, sc:0, fc:10,best_val: 2.2709734439849854\n",
      "92: length:0.0625, sc:0, fc:11,best_val: 2.2709734439849854\n",
      "93: length:0.0625, sc:0, fc:12,best_val: 2.2709734439849854\n",
      "94: length:0.0625, sc:0, fc:13,best_val: 2.2709734439849854\n",
      "95: length:0.0625, sc:0, fc:14,best_val: 2.2709734439849854\n",
      "96: length:0.0625, sc:0, fc:15,best_val: 2.2709734439849854\n",
      "97: length:0.0625, sc:0, fc:16,best_val: 2.2709734439849854\n",
      "98: length:0.0625, sc:0, fc:17,best_val: 2.2709734439849854\n",
      "99: length:0.0625, sc:0, fc:18,best_val: 2.2709734439849854\n",
      "100: length:0.0625, sc:0, fc:19,best_val: 2.2709734439849854\n",
      "101: length:0.03125, sc:0, fc:0,best_val: 2.2709734439849854\n",
      "102: length:0.03125, sc:0, fc:1,best_val: 2.2709734439849854\n",
      "103: length:0.03125, sc:0, fc:2,best_val: 2.2709734439849854\n",
      "104: length:0.03125, sc:0, fc:3,best_val: 2.2709734439849854\n",
      "105: length:0.03125, sc:0, fc:4,best_val: 2.2709734439849854\n",
      "106: length:0.03125, sc:0, fc:5,best_val: 2.2709734439849854\n",
      "107: length:0.03125, sc:0, fc:6,best_val: 2.2709734439849854\n",
      "108: length:0.03125, sc:0, fc:7,best_val: 2.2709734439849854\n",
      "109: length:0.03125, sc:0, fc:8,best_val: 2.2709734439849854\n",
      "110: length:0.03125, sc:0, fc:9,best_val: 2.2709734439849854\n",
      "111: length:0.03125, sc:0, fc:10,best_val: 2.2709734439849854\n",
      "112: length:0.03125, sc:0, fc:11,best_val: 2.2709734439849854\n",
      "113: length:0.03125, sc:0, fc:12,best_val: 2.2709734439849854\n",
      "114: length:0.03125, sc:0, fc:13,best_val: 2.2709734439849854\n",
      "115: length:0.03125, sc:0, fc:14,best_val: 2.2709734439849854\n",
      "116: length:0.03125, sc:0, fc:15,best_val: 2.2709734439849854\n",
      "117: length:0.03125, sc:0, fc:16,best_val: 2.2709734439849854\n",
      "118: length:0.03125, sc:0, fc:17,best_val: 2.2709734439849854\n",
      "119: length:0.03125, sc:0, fc:18,best_val: 2.2709734439849854\n",
      "120: length:0.03125, sc:0, fc:19,best_val: 2.2709734439849854\n",
      "121: length:0.015625, sc:0, fc:0,best_val: 2.2709734439849854\n",
      "122: length:0.015625, sc:0, fc:1,best_val: 2.2709734439849854\n",
      "123: length:0.015625, sc:0, fc:2,best_val: 2.2709734439849854\n",
      "124: length:0.015625, sc:0, fc:3,best_val: 2.2709734439849854\n",
      "125: length:0.015625, sc:0, fc:4,best_val: 2.2709734439849854\n",
      "126: length:0.015625, sc:0, fc:5,best_val: 2.2709734439849854\n",
      "127: length:0.015625, sc:0, fc:6,best_val: 2.2709734439849854\n",
      "128: length:0.015625, sc:0, fc:7,best_val: 2.2709734439849854\n",
      "129: length:0.015625, sc:0, fc:8,best_val: 2.2709734439849854\n",
      "130: length:0.015625, sc:0, fc:9,best_val: 2.2709734439849854\n",
      "131: length:0.015625, sc:0, fc:10,best_val: 2.2709734439849854\n",
      "132: length:0.015625, sc:0, fc:11,best_val: 2.2709734439849854\n",
      "133: length:0.015625, sc:0, fc:12,best_val: 2.2709734439849854\n",
      "134: length:0.015625, sc:0, fc:13,best_val: 2.2709734439849854\n",
      "135: length:0.015625, sc:0, fc:14,best_val: 2.2709734439849854\n",
      "136: length:0.015625, sc:0, fc:15,best_val: 2.2709734439849854\n",
      "137: length:0.015625, sc:0, fc:16,best_val: 2.2709734439849854\n",
      "138: length:0.015625, sc:0, fc:17,best_val: 2.2709734439849854\n",
      "139: length:0.015625, sc:0, fc:18,best_val: 2.2709734439849854\n",
      "140: length:0.015625, sc:0, fc:19,best_val: 2.2709734439849854\n",
      "141: length:0.0078125, sc:0, fc:0,best_val: 2.2709734439849854\n",
      "142: length:0.0078125, sc:0, fc:1,best_val: 2.2709734439849854\n",
      "143: length:0.0078125, sc:0, fc:2,best_val: 2.2709734439849854\n",
      "144: length:0.0078125, sc:0, fc:3,best_val: 2.2709734439849854\n",
      "145: length:0.0078125, sc:0, fc:4,best_val: 2.2709734439849854\n",
      "146: length:0.0078125, sc:0, fc:5,best_val: 2.2709734439849854\n",
      "147: length:0.0078125, sc:0, fc:6,best_val: 2.2709734439849854\n",
      "148: length:0.0078125, sc:0, fc:7,best_val: 2.2709734439849854\n",
      "149: length:0.0078125, sc:0, fc:8,best_val: 2.2709734439849854\n",
      "150: length:0.0078125, sc:0, fc:9,best_val: 2.2709734439849854\n",
      "151: length:0.0078125, sc:0, fc:10,best_val: 2.2709734439849854\n",
      "152: length:0.0078125, sc:0, fc:11,best_val: 2.2709734439849854\n",
      "153: length:0.0078125, sc:0, fc:12,best_val: 2.2709734439849854\n",
      "154: length:0.0078125, sc:0, fc:13,best_val: 2.2709734439849854\n",
      "155: length:0.0078125, sc:0, fc:14,best_val: 2.2709734439849854\n",
      "156: length:0.0078125, sc:0, fc:15,best_val: 2.2709734439849854\n",
      "157: length:0.0078125, sc:0, fc:16,best_val: 2.2709734439849854\n",
      "158: length:0.0078125, sc:0, fc:17,best_val: 2.2709734439849854\n",
      "159: length:0.0078125, sc:0, fc:18,best_val: 2.2709734439849854\n",
      "160: length:0.0078125, sc:1, fc:0,best_val: 2.219080686569214\n",
      "161: length:0.0078125, sc:2, fc:0,best_val: 1.9987263679504395\n",
      "162: length:0.0078125, sc:3, fc:0,best_val: 1.5664892196655273\n",
      "163: length:0.0078125, sc:4, fc:0,best_val: 0.3191484212875366\n",
      "164: length:0.0078125, sc:5, fc:0,best_val: 0.06640408933162689\n",
      "165: length:0.0078125, sc:0, fc:1,best_val: 0.06640408933162689\n",
      "166: length:0.0078125, sc:0, fc:2,best_val: 0.06640408933162689\n",
      "167: length:0.0078125, sc:0, fc:3,best_val: 0.06640408933162689\n",
      "168: length:0.0078125, sc:0, fc:4,best_val: 0.06640408933162689\n",
      "169: length:0.0078125, sc:0, fc:5,best_val: 0.06640408933162689\n",
      "170: length:0.0078125, sc:0, fc:6,best_val: 0.06640408933162689\n",
      "171: length:0.0078125, sc:0, fc:7,best_val: 0.06640408933162689\n",
      "172: length:0.0078125, sc:0, fc:8,best_val: 0.06640408933162689\n",
      "173: length:0.0078125, sc:0, fc:9,best_val: 0.06640408933162689\n",
      "174: length:0.0078125, sc:0, fc:10,best_val: 0.06640408933162689\n",
      "175: length:0.0078125, sc:0, fc:11,best_val: 0.06640408933162689\n",
      "176: length:0.0078125, sc:0, fc:12,best_val: 0.06640408933162689\n",
      "177: length:0.0078125, sc:1, fc:0,best_val: 0.02929241582751274\n",
      "178: length:0.0078125, sc:2, fc:0,best_val: 0.020920833572745323\n",
      "179: length:0.0078125, sc:0, fc:1,best_val: 0.020920833572745323\n",
      "180: length:0.0078125, sc:0, fc:2,best_val: 0.020920833572745323\n",
      "181: length:0.0078125, sc:0, fc:3,best_val: 0.020920833572745323\n",
      "182: length:0.0078125, sc:1, fc:0,best_val: 0.012590642087161541\n",
      "183: length:0.0078125, sc:2, fc:0,best_val: 0.006160233169794083\n",
      "184: length:0.0078125, sc:0, fc:1,best_val: 0.006160233169794083\n",
      "185: length:0.0078125, sc:0, fc:2,best_val: 0.006160233169794083\n",
      "186: length:0.0078125, sc:0, fc:3,best_val: 0.006160233169794083\n",
      "187: length:0.0078125, sc:0, fc:4,best_val: 0.006160233169794083\n",
      "188: length:0.0078125, sc:0, fc:5,best_val: 0.006160233169794083\n",
      "189: length:0.0078125, sc:0, fc:6,best_val: 0.006160233169794083\n",
      "190: length:0.0078125, sc:0, fc:7,best_val: 0.006160233169794083\n",
      "191: length:0.0078125, sc:0, fc:8,best_val: 0.006160233169794083\n",
      "192: length:0.0078125, sc:0, fc:9,best_val: 0.006160233169794083\n",
      "193: length:0.0078125, sc:0, fc:10,best_val: 0.006160233169794083\n",
      "194: length:0.0078125, sc:0, fc:11,best_val: 0.006160233169794083\n",
      "195: length:0.0078125, sc:1, fc:0,best_val: 0.005832047667354345\n",
      "196: length:0.0078125, sc:0, fc:1,best_val: 0.005832047667354345\n",
      "197: length:0.0078125, sc:0, fc:2,best_val: 0.005832047667354345\n",
      "198: length:0.0078125, sc:0, fc:3,best_val: 0.005832047667354345\n",
      "199: length:0.0078125, sc:0, fc:4,best_val: 0.005832047667354345\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(X.generator.turbo_state.failure_tolerance)\n",
    "for i in range(200):\n",
    "    print(f\"{i}: length:{X.generator.turbo_state.length}, \"\n",
    "          f\"sc:{X.generator.turbo_state.success_counter}, \"\n",
    "          f\"fc:{X.generator.turbo_state.failure_counter},\"\n",
    "          f\"best_val: {X.generator.turbo_state.best_value}\"\n",
    "          )\n",
    "    # do the optimization step\n",
    "    X.step()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "           x0        x1        x2        x3        x4        x5        x6  \\\n1    0.422018 -0.026224  0.718189  0.310911  0.707492  0.434676 -0.849791   \n2   -0.882145 -0.562799  0.733046  0.649420  0.283789  0.704892  0.618188   \n3    0.721131  0.314384 -0.289785 -0.852108 -0.844845 -0.384828  0.813833   \n4    0.980294 -0.875082  0.657125 -0.737197  0.140548 -0.459261  0.165984   \n5   -0.279773 -0.499483  0.918083  0.838570 -0.271987  0.706489 -0.546202   \n..        ...       ...       ...       ...       ...       ...       ...   \n201 -0.022770  0.010683 -0.029797 -0.007162 -0.024295 -0.008045  0.049794   \n202 -0.022540 -0.022878  0.003643 -0.007380 -0.009710 -0.008285  0.049588   \n203  0.008578  0.012142 -0.029341 -0.007557 -0.023872 -0.030718  0.039926   \n204 -0.022153 -0.022446  0.003245 -0.007724 -0.023675 -0.008701  0.019620   \n205  0.008259 -0.022299  0.003105 -0.007849 -0.023539 -0.008841  0.049104   \n\n           x7        x8        x9  ...       x13       x14       x15  \\\n1    0.970433 -0.413621  0.716674  ...  0.824777 -0.944750 -0.944922   \n2   -0.841374  0.417714 -0.108155  ...  0.325305 -0.264091  0.458591   \n3    0.312714  0.713930 -0.797754  ... -0.379077 -0.007176  0.454549   \n4   -0.273056  0.499388 -0.598485  ... -0.289586 -0.020493  0.948895   \n5    0.324510  0.836784  0.985140  ... -0.323156 -0.645561 -0.708913   \n..        ...       ...       ...  ...       ...       ...       ...   \n201  0.012742 -0.005660 -0.028956  ...  0.014557  0.009588 -0.024531   \n202 -0.018762  0.028848 -0.007280  ...  0.014784 -0.028681 -0.024309   \n203 -0.018570  0.028628 -0.028548  ...  0.014977  0.009129  0.005933   \n204  0.012140  0.028431 -0.028373  ...  0.015177  0.008901  0.005748   \n205 -0.018258  0.028288 -0.028248  ...  0.015310  0.008739 -0.017811   \n\n          x16       x17       x18       x19             f  xopt_runtime  \\\n1   -0.199756 -0.873053  0.873657  0.466680      9.041967      0.000166   \n2    0.101729  0.163075 -0.542988 -0.334861      5.862751      0.000035   \n3    0.444502 -0.237513  0.553099 -0.925864      7.220641      0.000025   \n4   -0.658809  0.005012  0.604305 -0.185447      5.603789      0.000025   \n5   -0.155306 -0.549053 -0.482431  0.728909     7.1246204      0.000024   \n..        ...       ...       ...       ...           ...           ...   \n201  0.023701  0.010039 -0.031757 -0.002574   0.010350179      0.000152   \n202  0.023450  0.010280 -0.031526 -0.024425     0.0111426      0.000072   \n203  0.023250  0.010498  0.000389 -0.024229    0.00869617      0.000050   \n204  0.023044  0.010694 -0.031128 -0.024057  0.0067548505      0.000047   \n205 -0.009901  0.010835 -0.030994 -0.023929   0.010678083      0.000066   \n\n     xopt_error  \n1         False  \n2         False  \n3         False  \n4         False  \n5         False  \n..          ...  \n201       False  \n202       False  \n203       False  \n204       False  \n205       False  \n\n[205 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0</th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>x4</th>\n      <th>x5</th>\n      <th>x6</th>\n      <th>x7</th>\n      <th>x8</th>\n      <th>x9</th>\n      <th>...</th>\n      <th>x13</th>\n      <th>x14</th>\n      <th>x15</th>\n      <th>x16</th>\n      <th>x17</th>\n      <th>x18</th>\n      <th>x19</th>\n      <th>f</th>\n      <th>xopt_runtime</th>\n      <th>xopt_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.422018</td>\n      <td>-0.026224</td>\n      <td>0.718189</td>\n      <td>0.310911</td>\n      <td>0.707492</td>\n      <td>0.434676</td>\n      <td>-0.849791</td>\n      <td>0.970433</td>\n      <td>-0.413621</td>\n      <td>0.716674</td>\n      <td>...</td>\n      <td>0.824777</td>\n      <td>-0.944750</td>\n      <td>-0.944922</td>\n      <td>-0.199756</td>\n      <td>-0.873053</td>\n      <td>0.873657</td>\n      <td>0.466680</td>\n      <td>9.041967</td>\n      <td>0.000166</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.882145</td>\n      <td>-0.562799</td>\n      <td>0.733046</td>\n      <td>0.649420</td>\n      <td>0.283789</td>\n      <td>0.704892</td>\n      <td>0.618188</td>\n      <td>-0.841374</td>\n      <td>0.417714</td>\n      <td>-0.108155</td>\n      <td>...</td>\n      <td>0.325305</td>\n      <td>-0.264091</td>\n      <td>0.458591</td>\n      <td>0.101729</td>\n      <td>0.163075</td>\n      <td>-0.542988</td>\n      <td>-0.334861</td>\n      <td>5.862751</td>\n      <td>0.000035</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.721131</td>\n      <td>0.314384</td>\n      <td>-0.289785</td>\n      <td>-0.852108</td>\n      <td>-0.844845</td>\n      <td>-0.384828</td>\n      <td>0.813833</td>\n      <td>0.312714</td>\n      <td>0.713930</td>\n      <td>-0.797754</td>\n      <td>...</td>\n      <td>-0.379077</td>\n      <td>-0.007176</td>\n      <td>0.454549</td>\n      <td>0.444502</td>\n      <td>-0.237513</td>\n      <td>0.553099</td>\n      <td>-0.925864</td>\n      <td>7.220641</td>\n      <td>0.000025</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.980294</td>\n      <td>-0.875082</td>\n      <td>0.657125</td>\n      <td>-0.737197</td>\n      <td>0.140548</td>\n      <td>-0.459261</td>\n      <td>0.165984</td>\n      <td>-0.273056</td>\n      <td>0.499388</td>\n      <td>-0.598485</td>\n      <td>...</td>\n      <td>-0.289586</td>\n      <td>-0.020493</td>\n      <td>0.948895</td>\n      <td>-0.658809</td>\n      <td>0.005012</td>\n      <td>0.604305</td>\n      <td>-0.185447</td>\n      <td>5.603789</td>\n      <td>0.000025</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.279773</td>\n      <td>-0.499483</td>\n      <td>0.918083</td>\n      <td>0.838570</td>\n      <td>-0.271987</td>\n      <td>0.706489</td>\n      <td>-0.546202</td>\n      <td>0.324510</td>\n      <td>0.836784</td>\n      <td>0.985140</td>\n      <td>...</td>\n      <td>-0.323156</td>\n      <td>-0.645561</td>\n      <td>-0.708913</td>\n      <td>-0.155306</td>\n      <td>-0.549053</td>\n      <td>-0.482431</td>\n      <td>0.728909</td>\n      <td>7.1246204</td>\n      <td>0.000024</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>-0.022770</td>\n      <td>0.010683</td>\n      <td>-0.029797</td>\n      <td>-0.007162</td>\n      <td>-0.024295</td>\n      <td>-0.008045</td>\n      <td>0.049794</td>\n      <td>0.012742</td>\n      <td>-0.005660</td>\n      <td>-0.028956</td>\n      <td>...</td>\n      <td>0.014557</td>\n      <td>0.009588</td>\n      <td>-0.024531</td>\n      <td>0.023701</td>\n      <td>0.010039</td>\n      <td>-0.031757</td>\n      <td>-0.002574</td>\n      <td>0.010350179</td>\n      <td>0.000152</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>-0.022540</td>\n      <td>-0.022878</td>\n      <td>0.003643</td>\n      <td>-0.007380</td>\n      <td>-0.009710</td>\n      <td>-0.008285</td>\n      <td>0.049588</td>\n      <td>-0.018762</td>\n      <td>0.028848</td>\n      <td>-0.007280</td>\n      <td>...</td>\n      <td>0.014784</td>\n      <td>-0.028681</td>\n      <td>-0.024309</td>\n      <td>0.023450</td>\n      <td>0.010280</td>\n      <td>-0.031526</td>\n      <td>-0.024425</td>\n      <td>0.0111426</td>\n      <td>0.000072</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.008578</td>\n      <td>0.012142</td>\n      <td>-0.029341</td>\n      <td>-0.007557</td>\n      <td>-0.023872</td>\n      <td>-0.030718</td>\n      <td>0.039926</td>\n      <td>-0.018570</td>\n      <td>0.028628</td>\n      <td>-0.028548</td>\n      <td>...</td>\n      <td>0.014977</td>\n      <td>0.009129</td>\n      <td>0.005933</td>\n      <td>0.023250</td>\n      <td>0.010498</td>\n      <td>0.000389</td>\n      <td>-0.024229</td>\n      <td>0.00869617</td>\n      <td>0.000050</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>-0.022153</td>\n      <td>-0.022446</td>\n      <td>0.003245</td>\n      <td>-0.007724</td>\n      <td>-0.023675</td>\n      <td>-0.008701</td>\n      <td>0.019620</td>\n      <td>0.012140</td>\n      <td>0.028431</td>\n      <td>-0.028373</td>\n      <td>...</td>\n      <td>0.015177</td>\n      <td>0.008901</td>\n      <td>0.005748</td>\n      <td>0.023044</td>\n      <td>0.010694</td>\n      <td>-0.031128</td>\n      <td>-0.024057</td>\n      <td>0.0067548505</td>\n      <td>0.000047</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.008259</td>\n      <td>-0.022299</td>\n      <td>0.003105</td>\n      <td>-0.007849</td>\n      <td>-0.023539</td>\n      <td>-0.008841</td>\n      <td>0.049104</td>\n      <td>-0.018258</td>\n      <td>0.028288</td>\n      <td>-0.028248</td>\n      <td>...</td>\n      <td>0.015310</td>\n      <td>0.008739</td>\n      <td>-0.017811</td>\n      <td>-0.009901</td>\n      <td>0.010835</td>\n      <td>-0.030994</td>\n      <td>-0.023929</td>\n      <td>0.010678083</td>\n      <td>0.000066</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>205 rows Ã— 23 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the collected data\n",
    "X.data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the trust region"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-2.2022e-02, -2.2299e-02, -2.7696e-02,  5.9213e-03, -8.2131e-03,\n          1.5310e-02, -2.8104e-02, -2.3810e-02, -9.9007e-03,  1.0835e-02,\n         -3.0994e-02, -2.3929e-02, -2.9012e-02, -3.6670e-02, -2.3539e-02,\n         -4.1223e-02,  1.9745e-02, -1.8258e-02, -4.8507e-03, -2.8248e-02],\n        [ 8.2594e-03,  1.1771e-02,  3.1203e-03,  3.6883e-02,  2.3715e-02,\n          4.6081e-02,  8.7390e-03,  5.6220e-03,  2.2900e-02,  4.3179e-02,\n          5.7626e-05,  5.9942e-03,  3.1050e-03, -7.8491e-03,  7.5748e-03,\n         -8.8412e-03,  4.9104e-02,  1.2008e-02,  2.8288e-02,  5.2454e-04]],\n       dtype=torch.float64)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.generator.trust_region"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customizing optimization\n",
    "Each generator has a set of options that can be modified to effect optimization behavior"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'optim': {'num_restarts': 20,\n  'raw_samples': 20,\n  'sequential': True,\n  'max_travel_distances': None,\n  'use_turbo': True},\n 'acq': {'proximal_lengthscales': None,\n  'use_transformed_proximal_weights': True,\n  'monte_carlo_samples': 128,\n  'beta': 2.0},\n 'model': {'name': 'standard',\n  'custom_constructor': None,\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {}},\n 'n_initial': 5,\n 'use_cuda': False}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.generator.options.dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# example: add a Gamma(1.0,10.0) prior to the noise hyperparameter to reduce model noise\n",
    "# (good for optimizing noise-free simulations)\n",
    "X.generator.options.model.use_low_noise_prior = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-20.7809], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(2.3985, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(-1.5826, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.4325, 0.6182, 0.4595, 0.4668, 0.5147, 0.4572, 0.7468, 0.3890, 0.5573,\n           0.5351, 0.4713, 0.4142, 0.5240, 0.3572, 0.4744, 0.5370, 0.3852, 0.4317,\n           0.5737, 0.3547]], dtype=torch.float64, requires_grad=True))]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X.generator.model.named_parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare to normal UCB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "evaluator = Evaluator(function=evaluate_sphere)\n",
    "options = UpperConfidenceBoundGenerator.default_options()\n",
    "options.n_initial = 5\n",
    "options.optim.use_turbo = False\n",
    "generator = UpperConfidenceBoundGenerator(vocs, options)\n",
    "\n",
    "X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.70230961]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.57788754]\n",
      "[4.24313593]\n",
      "[4.24313593]\n",
      "[4.24313593]\n",
      "[4.24313593]\n",
      "[4.24313593]\n",
      "[4.24313593]\n",
      "[4.24313593]\n",
      "[4.24313593]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[4.11975861]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.09187317]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[3.08557582]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.68867469]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n",
      "[2.61213923]\n"
     ]
    }
   ],
   "source": [
    "X.step()\n",
    "\n",
    "for i in range(200):\n",
    "    print(f\"{X.vocs.objective_data(X.data).min().to_numpy()}\")\n",
    "    # do the optimization step\n",
    "    X.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-18.6872], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(4.2172, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(0.1444, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.5190, 0.5267, 0.5178, 0.6047, 0.4322, 0.5229, 0.4061, 0.4835, 0.4625,\n           0.3935, 0.4572, 0.2893, 0.4270, 0.4062, 0.4094, 0.4952, 0.3312, 0.4645,\n           0.5173, 0.4439]], dtype=torch.float64, requires_grad=True))]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X.generator.model.named_parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
