{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from botorch.test_functions.multi_objective import BraninCurrin\n",
    "\n",
    "\n",
    "problem = BraninCurrin(negate=True).to(**tkwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Power\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "\n",
    "NOISE_SE = torch.tensor([15.19, 0.63], **tkwargs)\n",
    "\n",
    "def generate_initial_data(n=6):\n",
    "    # generate training data\n",
    "    train_x = draw_sobol_samples(bounds=problem.bounds,n=n, q=1).squeeze(1)\n",
    "    train_obj_true = problem(train_x)\n",
    "    train_obj = train_obj_true + torch.randn_like(train_obj_true) * NOISE_SE\n",
    "    return train_x, train_obj, train_obj_true\n",
    "\n",
    "\n",
    "def initialize_model(train_x, train_obj):\n",
    "    # define models for objective and constraint\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "    models = []\n",
    "    for i in range(train_obj.shape[-1]):\n",
    "        train_y = train_obj[..., i:i+1]\n",
    "        train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
    "        models.append(\n",
    "            SingleTaskGP(train_x, train_y, outcome_transform=Power(2.0))\n",
    "        )\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import FastNondominatedPartitioning\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement, qNoisyExpectedHypervolumeImprovement\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
    "\n",
    "standard_bounds = torch.zeros(2, problem.dim, **tkwargs)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, problem.bounds)).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=problem.ref_point,\n",
    "        Y=pred,\n",
    "    )\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values\n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point.tolist(),  # use known reference point\n",
    "        X_baseline=normalize(train_x, problem.bounds),\n",
    "        prune_baseline=True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values\n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "\n",
    "\n",
    "def optimize_qnparego_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Samples a set of random weights for each candidate in the batch, performs sequential greedy optimization\n",
    "    of the qNParEGO acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(train_x).mean\n",
    "    acq_func_list = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        weights = sample_simplex(problem.num_objectives, **tkwargs).squeeze()\n",
    "        objective = GenericMCObjective(get_chebyshev_scalarization(weights=weights, Y=pred))\n",
    "        acq_func = qNoisyExpectedImprovement(  # pyre-ignore: [28]\n",
    "            model=model,\n",
    "            objective=objective,\n",
    "            X_baseline=train_x,\n",
    "            sampler=sampler,\n",
    "            prune_baseline=True,\n",
    "        )\n",
    "        acq_func_list.append(acq_func)\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf_list(\n",
    "        acq_function_list=acq_func_list,\n",
    "        bounds=standard_bounds,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    )\n",
    "    # observe new values\n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "No mean transform provided.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 76\u001B[0m\n\u001B[0;32m     69\u001B[0m qnehvi_sampler \u001B[38;5;241m=\u001B[39m SobolQMCNormalSampler(sample_shape\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mSize([MC_SAMPLES]))\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m# optimize acquisition functions and get new observations\u001B[39;00m\n\u001B[0;32m     72\u001B[0m (\n\u001B[0;32m     73\u001B[0m     new_x_qparego,\n\u001B[0;32m     74\u001B[0m     new_obj_qparego,\n\u001B[0;32m     75\u001B[0m     new_obj_true_qparego,\n\u001B[1;32m---> 76\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[43moptimize_qnparego_and_get_observation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_qparego\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_x_qparego\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_obj_qparego\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqparego_sampler\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m new_x_qehvi, new_obj_qehvi, new_obj_true_qehvi \u001B[38;5;241m=\u001B[39m optimize_qehvi_and_get_observation(\n\u001B[0;32m     80\u001B[0m     model_qehvi, train_x_qehvi, train_obj_qehvi, qehvi_sampler\n\u001B[0;32m     81\u001B[0m )\n\u001B[0;32m     82\u001B[0m (\n\u001B[0;32m     83\u001B[0m     new_x_qnehvi,\n\u001B[0;32m     84\u001B[0m     new_obj_qnehvi,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     87\u001B[0m     model_qnehvi, train_x_qnehvi, train_obj_qnehvi, qnehvi_sampler\n\u001B[0;32m     88\u001B[0m )\n",
      "Cell \u001B[1;32mIn[6], line 9\u001B[0m, in \u001B[0;36moptimize_qnparego_and_get_observation\u001B[1;34m(model, train_x, train_obj, sampler)\u001B[0m\n\u001B[0;32m      7\u001B[0m train_x \u001B[38;5;241m=\u001B[39m normalize(train_x, problem\u001B[38;5;241m.\u001B[39mbounds)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m----> 9\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposterior\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\n\u001B[0;32m     10\u001B[0m acq_func_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(BATCH_SIZE):\n",
      "File \u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\xopt\\lib\\site-packages\\botorch\\posteriors\\posterior_list.py:144\u001B[0m, in \u001B[0;36mPosteriorList.mean\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"The mean of the posterior as a `(b) x n x m`-dim Tensor.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m    This is only supported if all posteriors provide a mean.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reshape_and_cat(tensors\u001B[38;5;241m=\u001B[39m[p\u001B[38;5;241m.\u001B[39mmean \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposteriors])\n",
      "File \u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\xopt\\lib\\site-packages\\botorch\\posteriors\\posterior_list.py:144\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"The mean of the posterior as a `(b) x n x m`-dim Tensor.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m    This is only supported if all posteriors provide a mean.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reshape_and_cat(tensors\u001B[38;5;241m=\u001B[39m[\u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposteriors])\n",
      "File \u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\xopt\\lib\\site-packages\\botorch\\posteriors\\transformed.py:87\u001B[0m, in \u001B[0;36mTransformedPosterior.mean\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"The mean of the posterior as a `batch_shape x n x m`-dim Tensor.\"\"\"\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mean_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo mean transform provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     89\u001B[0m     variance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_posterior\u001B[38;5;241m.\u001B[39mvariance\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: No mean transform provided."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import (\n",
    "    DominatedPartitioning,\n",
    ")\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "N_BATCH = 20 if not SMOKE_TEST else 10\n",
    "MC_SAMPLES = 128 if not SMOKE_TEST else 16\n",
    "\n",
    "verbose = True\n",
    "\n",
    "hvs_qparego, hvs_qehvi, hvs_qnehvi, hvs_random = [], [], [], []\n",
    "\n",
    "# call helper functions to generate initial training data and initialize model\n",
    "train_x_qparego, train_obj_qparego, train_obj_true_qparego = generate_initial_data(\n",
    "    n=2 * (problem.dim + 1)\n",
    ")\n",
    "mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "\n",
    "train_x_qehvi, train_obj_qehvi, train_obj_true_qehvi = (\n",
    "    train_x_qparego,\n",
    "    train_obj_qparego,\n",
    "    train_obj_true_qparego,\n",
    ")\n",
    "train_x_qnehvi, train_obj_qnehvi, train_obj_true_qnehvi = (\n",
    "    train_x_qparego,\n",
    "    train_obj_qparego,\n",
    "    train_obj_true_qparego,\n",
    ")\n",
    "train_x_random, train_obj_random, train_obj_true_random = (\n",
    "    train_x_qparego,\n",
    "    train_obj_qparego,\n",
    "    train_obj_true_qparego,\n",
    ")\n",
    "mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi)\n",
    "\n",
    "# compute hypervolume\n",
    "bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qparego)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "\n",
    "hvs_qparego.append(volume)\n",
    "hvs_qehvi.append(volume)\n",
    "hvs_qnehvi.append(volume)\n",
    "hvs_random.append(volume)\n",
    "\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(1, N_BATCH + 1):\n",
    "\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    # fit the models\n",
    "    fit_gpytorch_mll(mll_qparego)\n",
    "    fit_gpytorch_mll(mll_qehvi)\n",
    "    fit_gpytorch_mll(mll_qnehvi)\n",
    "\n",
    "    # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "    qparego_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "    qehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "    qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "\n",
    "    # optimize acquisition functions and get new observations\n",
    "    (\n",
    "        new_x_qparego,\n",
    "        new_obj_qparego,\n",
    "        new_obj_true_qparego,\n",
    "    ) = optimize_qnparego_and_get_observation(\n",
    "        model_qparego, train_x_qparego, train_obj_qparego, qparego_sampler\n",
    "    )\n",
    "    new_x_qehvi, new_obj_qehvi, new_obj_true_qehvi = optimize_qehvi_and_get_observation(\n",
    "        model_qehvi, train_x_qehvi, train_obj_qehvi, qehvi_sampler\n",
    "    )\n",
    "    (\n",
    "        new_x_qnehvi,\n",
    "        new_obj_qnehvi,\n",
    "        new_obj_true_qnehvi,\n",
    "    ) = optimize_qnehvi_and_get_observation(\n",
    "        model_qnehvi, train_x_qnehvi, train_obj_qnehvi, qnehvi_sampler\n",
    "    )\n",
    "    new_x_random, new_obj_random, new_obj_true_random = generate_initial_data(\n",
    "        n=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # update training points\n",
    "    train_x_qparego = torch.cat([train_x_qparego, new_x_qparego])\n",
    "    train_obj_qparego = torch.cat([train_obj_qparego, new_obj_qparego])\n",
    "    train_obj_true_qparego = torch.cat([train_obj_true_qparego, new_obj_true_qparego])\n",
    "\n",
    "    train_x_qehvi = torch.cat([train_x_qehvi, new_x_qehvi])\n",
    "    train_obj_qehvi = torch.cat([train_obj_qehvi, new_obj_qehvi])\n",
    "    train_obj_true_qehvi = torch.cat([train_obj_true_qehvi, new_obj_true_qehvi])\n",
    "\n",
    "    train_x_qnehvi = torch.cat([train_x_qnehvi, new_x_qnehvi])\n",
    "    train_obj_qnehvi = torch.cat([train_obj_qnehvi, new_obj_qnehvi])\n",
    "    train_obj_true_qnehvi = torch.cat([train_obj_true_qnehvi, new_obj_true_qnehvi])\n",
    "\n",
    "    train_x_random = torch.cat([train_x_random, new_x_random])\n",
    "    train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
    "    train_obj_true_random = torch.cat([train_obj_true_random, new_obj_true_random])\n",
    "\n",
    "    # update progress\n",
    "    for hvs_list, train_obj in zip(\n",
    "        (hvs_random, hvs_qparego, hvs_qehvi, hvs_qnehvi),\n",
    "        (\n",
    "            train_obj_true_random,\n",
    "            train_obj_true_qparego,\n",
    "            train_obj_true_qehvi,\n",
    "            train_obj_true_qnehvi,\n",
    "        ),\n",
    "    ):\n",
    "        # compute hypervolume\n",
    "        bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj)\n",
    "        volume = bd.compute_hypervolume().item()\n",
    "        hvs_list.append(volume)\n",
    "\n",
    "    # reinitialize the models so they are ready for fitting on next iteration\n",
    "    # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "    # using the hyperparameters from the previous iteration\n",
    "    mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "    mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "    mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi)\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\nBatch {iteration:>2}: Hypervolume (random, qNParEGO, qEHVI, qNEHVI) = \"\n",
    "            f\"({hvs_random[-1]:>4.2f}, {hvs_qparego[-1]:>4.2f}, {hvs_qehvi[-1]:>4.2f}, {hvs_qnehvi[-1]:>4.2f}), \"\n",
    "            f\"time = {t1-t0:>4.2f}.\",\n",
    "            end=\"\",\n",
    "        )\n",
    "    else:\n",
    "        print(\".\", end=\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
